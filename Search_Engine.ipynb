{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Βήμα 1. Συλλογή δεδομένων:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://en.wikipedia.org/wiki/Processor_(computing)\n",
      "Fetching: https://en.wikipedia.org//wiki/Processor_(computing)\n",
      "Fetching: https://en.wikipedia.org//wiki/Processor_(disambiguation)#Computing\n",
      "Fetching: https://en.wikipedia.org//wiki/Computing\n",
      "Fetching: https://en.wikipedia.org//wiki/Computer_science\n",
      "Fetching: https://en.wikipedia.org//wiki/Circuit_(computer_science)\n",
      "Fetching: https://en.wikipedia.org//wiki/Memory_(computing)\n",
      "Fetching: https://en.wikipedia.org//wiki/Microprocessor\n",
      "Fetching: https://en.wikipedia.org//wiki/Metal%E2%80%93oxide%E2%80%93semiconductor\n",
      "The data has been saved to \"articles.json\"\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "#Συνάρτηση για να ανακτήσουμε συγκεκριμένα άρθρα\n",
    "def fetch_articles(start_url, num_articles = 10):\n",
    "    base_url = \"https://en.wikipedia.org/\"\n",
    "    visited = set() #Σύνολο url τα οποία έχουμε ήδη επισκεφτεί\n",
    "    articles = [] #Δεδομένα των άρθρων\n",
    "    \n",
    "    #Συνάρτηση για να συλλέξουμε τα δεδομένα της κάθε σελίδας\n",
    "    def fetch_article_data(url):\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        title = soup.find('h1').text\n",
    "        paragraphs = soup.find_all('p')\n",
    "        content = \" \".join([p.text for p in paragraphs])\n",
    "        return {\"title\": title, \"url\": url, \"content\": content}\n",
    "    \n",
    "    #Τα url τα οποία θα επισκεφτούμε στην συνέχεια\n",
    "    to_visit = [start_url]\n",
    "    while to_visit and len(articles) < num_articles:\n",
    "        current_url = to_visit.pop(0)\n",
    "        if current_url in visited:\n",
    "            continue\n",
    "        visited.add(current_url)\n",
    "        \n",
    "        #Αποφεύγουμε το Main Page της Wikipedia\n",
    "        if 'Main_Page' in current_url:\n",
    "            continue\n",
    "        \n",
    "        print(f\"Fetching: {current_url}\")\n",
    "        article = fetch_article_data(current_url)\n",
    "        if article:\n",
    "            articles.append(article)\n",
    "        \n",
    "        response = requests.get(current_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for link in soup.find_all('a', href = True):\n",
    "            href = link['href']\n",
    "            if href.startswith('/wiki/') and ':' not in href:\n",
    "                full_url = base_url + href\n",
    "                if full_url not in visited:\n",
    "                    to_visit.append(full_url)\n",
    "        \n",
    "    return articles\n",
    "        \n",
    "\n",
    "start_url = \"https://en.wikipedia.org/wiki/Processor_(computing)\"\n",
    "articles = fetch_articles(start_url, num_articles = 9)\n",
    "\n",
    "with open(\"articles.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(articles, file, ensure_ascii=False, indent=4)\n",
    "print(\"The data has been saved to \\\"articles.json\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Βήμα 2. Προεπεξεργασία κειμένου (Text Processing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Μαρία\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Μαρία\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Μαρία\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has been saved to \"processed.json\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Συνάρτηση για επεξεργασία κειμένου\n",
    "def process_content(text):\n",
    "    if not isinstance(text, str):  # Έλεγχος ότι η είσοδος είναι string\n",
    "        return text\n",
    "    \n",
    "    # Tokenization\n",
    "    words = word_tokenize(text)\n",
    "    processed_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word.isdigit():  # Αν είναι αριθμός, το προσθέτουμε απευθείας\n",
    "            processed_words.append(word)\n",
    "        else:\n",
    "            # Αφαίρεση ειδικών χαρακτήρων\n",
    "            word = word.strip(string.punctuation)\n",
    "            \n",
    "            # Lemmatization\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "            \n",
    "            # Αφαίρεση stop words\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            if word.lower() not in stop_words:\n",
    "                processed_words.append(word.lower())\n",
    "    \n",
    "    return ' '.join(processed_words)\n",
    "\n",
    "# Ανάγνωση δεδομένων από αρχείο JSON\n",
    "try:\n",
    "    dataframe = pd.read_json(\"articles.json\")  # διαδρομή αρχείου\n",
    "except Exception as e:\n",
    "    print(f\"Error reading JSON file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Επεξεργασία δεδομένων DataFrame\n",
    "for column in dataframe.columns:\n",
    "    dataframe[column] = dataframe[column].apply(process_content)\n",
    "\n",
    "# Μετονομασία στηλών\n",
    "new_columns = {\n",
    "    dataframe.columns[0]: 'Title',\n",
    "    dataframe.columns[1]: 'Url',\n",
    "    dataframe.columns[2]: 'Content'\n",
    "}\n",
    "dataframe = dataframe.rename(columns=new_columns)\n",
    "\n",
    "# Αποθήκευση επεξεργασμένων δεδομένων σε νέο αρχείο JSON\n",
    "dataframe.to_json(\"processed.json\", orient='records', indent=4)\n",
    "\n",
    "print(\"The data has been saved to \\\"processed.json\\\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Βήμα 3. Ευρετήριο (indexing): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has been saved to \"inverted_index.json\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Διαβάζουμε τα επεξεργασμένα δεδομένα από το αρχείο processed.json\n",
    "with open('processed.json', 'r') as json_file:\n",
    "    documents = json.load(json_file)  # Φορτώνουμε το JSON σε μορφή λίστας/λεξικού\n",
    "\n",
    "# Αντίστροφο ευρετήριο (inverted index)\n",
    "inverted_index = defaultdict(list)  # Χρησιμοποιούμε defaultdict για να αποθηκεύσουμε τα IDs των εγγράφων\n",
    "\n",
    "# Επεξεργασία κάθε έγγραφου\n",
    "for doc_id, document in enumerate(documents):\n",
    "    # Σπλιτάρω και αποθηκεύω στο terms όλα τα κομμάτια\n",
    "    for field, field_value in document.items():\n",
    "        if isinstance(field_value, str):  # Ελέγχω αν το πεδίο είναι string\n",
    "            terms = field_value.split()  # Χωρίζω το κείμενο σε όρους\n",
    "            \n",
    "            # Δημιουργία του αντεστραμμένου ευρετηρίου (αντί για αριθμό εμφανίσεων, αποθηκεύουμε τα doc_id)\n",
    "            for term in terms:\n",
    "                if doc_id not in inverted_index[term]:  # Ελέγχουμε αν το doc_id είναι ήδη στη λίστα\n",
    "                    inverted_index[term].append(doc_id)\n",
    "\n",
    "# Αποθήκευση του αντεστραμμένου ευρετηρίου σε αρχείο JSON\n",
    "with open('inverted_index.json', 'w') as json_file:\n",
    "    json.dump(inverted_index, json_file, indent=4)  \n",
    "\n",
    "\n",
    "print(\"The data has been saved to \\\"inverted_index.json\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Βήμα 4. Μηχανή αναζήτησης (Search Engine):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " α) Επεξεργασία ερωτήματος (Query Processing): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Φόρτωση ανεστραμμένου ευρετηρίου\n",
    "def load_inverted_index(filepath):\n",
    "    with open(filepath, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Boolean Search Function\n",
    "def boolean_search(query, inverted_index):\n",
    "    # Διαχωρισμός όρων και τελεστών\n",
    "    terms = query.lower().split()  # Διαχωρισμός βάσει κενού\n",
    "    if not terms:\n",
    "        return set()  # Επιστρέφει άδειο σύνολο αν το ερώτημα είναι κενό\n",
    "\n",
    "    result_set = set(inverted_index.get(terms[0], []))  # Ξεκινάμε με τον πρώτο όρο\n",
    "    \n",
    "    i = 1\n",
    "    while i < len(terms):\n",
    "        operator = terms[i]  # AND, OR, NOT\n",
    "        \n",
    "        # Επαλήθευση αν υπάρχει ο επόμενος όρος\n",
    "        if i + 1 >= len(terms):\n",
    "            print(f\"Λάθος: Λείπει όρος μετά τον τελεστή '{operator}'\")\n",
    "            break\n",
    "        \n",
    "        next_term = terms[i + 1]\n",
    "        next_docs = set(inverted_index.get(next_term, []))  # Έγγραφα για τον επόμενο όρο\n",
    "        \n",
    "        # Εκτέλεση της λειτουργίας\n",
    "        if operator == \"and\":\n",
    "            result_set = result_set.intersection(next_docs)\n",
    "        elif operator == \"or\":\n",
    "            result_set = result_set.union(next_docs)\n",
    "        elif operator == \"not\":\n",
    "            result_set = result_set.difference(next_docs)\n",
    "        else:\n",
    "            print(f\"Λάθος: Άγνωστος τελεστής '{operator}'\")\n",
    "            break\n",
    "        \n",
    "        i += 2  # Επόμενος τελεστής ή όρος\n",
    "    \n",
    "    return result_set\n",
    "\n",
    "# main Συνάρτηση\n",
    "def main():\n",
    "    # Φόρτωσε το ανεστραμμένο ευρετήριο\n",
    "    filepath = \"inverted_index.json\"  \n",
    "    inverted_index = load_inverted_index(filepath)\n",
    "\n",
    "    print(\"Καλωσήρθατε στη Μηχανή Αναζήτησης!\")\n",
    "    print(\"Πληκτρολογήστε το ερώτημά σας χρησιμοποιώντας Boolean τελεστές (AND, OR, NOT) ή 'exit' για έξοδο.\")\n",
    "    \n",
    "    while True:\n",
    "        # Εισαγωγή ερωτήμ,ατος από τον χρήστη\n",
    "        query = input(\"\\nΕρώτημα: \").strip()\n",
    "        \n",
    "        # Έξοδος αν ο χρήστης πληκτρολογήσει \"exit\"\n",
    "        if query.lower() == \"exit\":\n",
    "            print(\"Έξοδος από τη Μηχανή Αναζήτησης!\")\n",
    "            break\n",
    "        \n",
    "        # Εκτέλεση Boolean Search\n",
    "        results = boolean_search(query, inverted_index)\n",
    "        \n",
    "        # Εμφάνιση αποτελεσμάτων\n",
    "        if results:\n",
    "            print(f\"Αποτελέσματα για το ερώτημα '{query}': {results}\")\n",
    "        else:\n",
    "            print(f\"Δεν βρέθηκαν αποτελέσματα για το ερώτημα '{query}'.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "β) Κατάταξη αποτελεσμάτων (Ranking):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Συνάρτηση για τη φόρτωση εγγράφων από ένα αρχείο JSON\n",
    "def load_documents(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "        # Αν το JSON περιέχει λίστα, επιστρέφουμε τα περιεχόμενα του πεδίου \"content\"\n",
    "        if isinstance(data, list):\n",
    "            return [doc[\"content\"] for doc in data if \"content\" in doc]\n",
    "        # Αν το JSON περιέχει λεξικό (π.χ. το ανεστραμμένο ευρετήριο), επιστρέφουμε το λεξικό\n",
    "        elif isinstance(data, dict):\n",
    "            return data\n",
    "        # Αν δεν είναι καμία από τις δύο μορφές, εμφανίζεται μήνυμα λάθους\n",
    "        else:\n",
    "            raise ValueError(\"Το αρχείο δεν έχει υποστηριζόμενη μορφή.\")\n",
    "\n",
    "# Boolean Search για αναζήτηση εγγράφων με βάση Boolean λογική (AND, OR, NOT)\n",
    "def boolean_search(query, inverted_index):\n",
    "    terms = query.lower().split()  # Διαχωρισμός του ερωτήματος σε λέξεις\n",
    "    if not terms:  # Αν το ερώτημα είναι κενό, επιστρέφουμε κενό σύνολο\n",
    "        return set()\n",
    "\n",
    "    # Ξεκινάμε με τα έγγραφα που περιέχουν την πρώτη λέξη\n",
    "    result_set = set(inverted_index.get(terms[0], []))\n",
    "    i = 1\n",
    "    while i < len(terms):  # Επεξεργασία των υπόλοιπων λέξεων και τελεστών\n",
    "        operator = terms[i]  # Παίρνουμε τον Boolean τελεστή (AND, OR, NOT)\n",
    "        if i + 1 >= len(terms):  # Αν δεν υπάρχει άλλη λέξη μετά τον τελεστή, σταματάμε\n",
    "            break\n",
    "        next_term = terms[i + 1]  # Επόμενη λέξη\n",
    "        next_docs = set(inverted_index.get(next_term, []))  # Έγγραφα που περιέχουν τη λέξη\n",
    "        # Εκτέλεση της αντίστοιχης Boolean λειτουργίας\n",
    "        if operator == \"and\":\n",
    "            result_set = result_set.intersection(next_docs)\n",
    "        elif operator == \"or\":\n",
    "            result_set = result_set.union(next_docs)\n",
    "        elif operator == \"not\":\n",
    "            result_set = result_set.difference(next_docs)\n",
    "        i += 2  # Προχωράμε στον επόμενο τελεστή/λέξη\n",
    "    return result_set\n",
    "\n",
    "# TF-IDF Ranking για κατάταξη εγγράφων με βάση τη σχετικότητα (TF-IDF score)\n",
    "def tfidf_ranking(query, documents):\n",
    "    vectorizer = TfidfVectorizer()  # Δημιουργία του TF-IDF Vectorizer\n",
    "    doc_vectors = vectorizer.fit_transform(documents)  # Υπολογισμός TF-IDF για τα έγγραφα\n",
    "    query_vector = vectorizer.transform([query.lower()])  # Μετατροπή του ερωτήματος σε διάνυσμα\n",
    "    scores = cosine_similarity(query_vector, doc_vectors).flatten()  # Υπολογισμός cosine similarity\n",
    "    sorted_indices = np.argsort(scores)[::-1]  # Ταξινόμηση των εγγράφων με βάση τη βαθμολογία\n",
    "    # Επιστροφή μόνο των εγγράφων με βαθμολογία > 0\n",
    "    return [(idx, scores[idx]) for idx in sorted_indices if scores[idx] > 0.0]\n",
    "\n",
    "# BM25 Ranking για κατάταξη εγγράφων με βάση το BM25\n",
    "def bm25_ranking(query, documents):\n",
    "    tokenized_docs = [doc.split() for doc in documents]  # Διαχωρισμός των εγγράφων σε λέξεις\n",
    "    bm25 = BM25Okapi(tokenized_docs)  # Δημιουργία BM25 μοντέλου\n",
    "    tokenized_query = query.split()  # Διαχωρισμός του ερωτήματος σε λέξεις\n",
    "    scores = bm25.get_scores(tokenized_query)  # Υπολογισμός BM25 scores\n",
    "    sorted_indices = np.argsort(scores)[::-1]  # Ταξινόμηση των εγγράφων με βάση τη βαθμολογία\n",
    "    # Επιστροφή των εγγράφων με βαθμολογίες\n",
    "    return [(idx, scores[idx]) for idx in sorted_indices if scores[idx] > 0.0]\n",
    "\n",
    "# Κύρια συνάρτηση για την εκτέλεση της μηχανής αναζήτησης\n",
    "def main():\n",
    "    # Αρχεία εισόδου\n",
    "    filepath_docs = \"articles.json\"  # Αρχείο με τα έγγραφα\n",
    "    filepath_index = \"inverted_index.json\"  # Αρχείο με το ανεστραμμένο ευρετήριο\n",
    "    documents = load_documents(filepath_docs)  # Φόρτωση των εγγράφων\n",
    "    inverted_index = load_documents(filepath_index)  # Φόρτωση του ανεστραμμένου ευρετηρίου\n",
    "\n",
    "    # Εμφάνιση επιλογών στον χρήστη\n",
    "    print(\"Επιλέξτε αλγόριθμο ανάκτησης:\")\n",
    "    print(\"1. Boolean Retrieval\")\n",
    "    print(\"2. TF-IDF Ranking\")\n",
    "    print(\"3. BM25 Ranking\")\n",
    "    print(\"Πληκτρολογήστε 'exit' για έξοδο.\")\n",
    "\n",
    "    while True:\n",
    "        choice = input(\"\\nΕπιλογή: \").strip()  # Είσοδος επιλογής από τον χρήστη\n",
    "        if choice.lower() == \"exit\":  # Έξοδος από το πρόγραμμα\n",
    "            print(\"Έξοδος από τη Μηχανή Αναζήτησης!\")\n",
    "            break\n",
    "\n",
    "        query = input(\"Ερώτημα: \").strip()  # Είσοδος ερωτήματος\n",
    "        if not query:  # Αν το ερώτημα είναι κενό\n",
    "            print(\"Το ερώτημα δεν μπορεί να είναι κενό.\")\n",
    "            continue\n",
    "\n",
    "        if choice == \"1\":  # Boolean Retrieval\n",
    "            results = boolean_search(query, inverted_index)\n",
    "            if results:\n",
    "                print(\"\\nBoolean Results:\")\n",
    "                for idx in results:\n",
    "                    if idx < len(documents):  # Έλεγχος αν το έγγραφο υπάρχει\n",
    "                        print(f\"Έγγραφο {idx}\")\n",
    "                    else:\n",
    "                        print(f\"Το έγγραφο {idx} δεν υπάρχει στο articles.json.\")\n",
    "            else:\n",
    "                print(\"Δεν βρέθηκαν αποτελέσματα.\")\n",
    "\n",
    "        elif choice == \"2\":  # TF-IDF Ranking\n",
    "            results = tfidf_ranking(query, documents)\n",
    "            print(\"\\nTF-IDF Results:\")\n",
    "            for idx, score in results:\n",
    "                if idx < len(documents):  # Έλεγχος αν το έγγραφο υπάρχει\n",
    "                    print(f\"Έγγραφο {idx} (Σκορ: {score:.4f})\")\n",
    "                else:\n",
    "                    print(f\"Το έγγραφο {idx} δεν υπάρχει στο articles.json.\")\n",
    "\n",
    "        elif choice == \"3\":  # BM25 Ranking\n",
    "            results = bm25_ranking(query, documents)\n",
    "            print(\"\\nBM25 Results:\")\n",
    "            for idx, score in results:\n",
    "                if idx < len(documents):  # Έλεγχος αν το έγγραφο υπάρχει\n",
    "                    print(f\"Έγγραφο {idx} (Σκορ: {score:.4f})\")\n",
    "                else:\n",
    "                    print(f\"Το έγγραφο {idx} δεν υπάρχει στο articles.json.\")\n",
    "\n",
    "        else:  # Μη έγκυρη επιλογή\n",
    "            print(\"Μη έγκυρη επιλογή. Παρακαλώ προσπαθήστε ξανά.\")\n",
    "\n",
    "# Εκκίνηση προγράμματος\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
