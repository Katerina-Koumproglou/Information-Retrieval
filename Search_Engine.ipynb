{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Βήμα 1. Συλλογή δεδομένων:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Αρχικά, πρέπει να συμπεριλάβουμε στον κώδικα μας την βιβλιοθήκη requests έτσι ώστε να κάνουμε αιτήματρα HTTP.\n",
    "- Εισάγουμε από την βιβλιοθήκη bs4 το BeautifulSoup, το οποίο χρησιμοποιούμε για να αναλύσουμε το HTML μιας ιστοσελίδας, έτσι ώστε να εξάγουμε διάφορα δεδομένα που χρειαζόμαστε, όπως τίτλους και παραγράφους.\n",
    "- Ακόμη, εισάγουμε την βιβλιοθήκη json, η οποία χρησιμοποιείται για την διαχείριση δεδομένων σε μορφή JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Δημιουργούμε την συνάρτηση fetch_articles, σκοπός τη οποίας είναι να ανακτά τα δεδομένα των αρθρών από την ιστοσελίδα που θα ορίσουμε, ξεκινώντας από το start_url και μέχρι τον αριθμό των άρθρων που θέλουμε να συλλέξουμε num_articles, όπου στην συγκεκριμένη περίπτωση είναι 10 άρθρα.\n",
    "- Ορίζουμε την βάση διευθύνσεων από την οποία θα ξεκινήσουμε, δηλαδή την Wikipedia.\n",
    "- Στο σύνολο visited αποθηκεύουμε τα URLs που έχουμε ήδη επισκεφθεί, έτσι ώστε να μην τα επισκεφτούμε ξανά.\n",
    "- Δημιουργούμε την λίστα articles η οποία περιέχει τα δεδομένα των αρθρών που συλλέξαμε.\n",
    "- Σκοπός της συνάρτησης fetch_article_data είναι για κάθε άρθρο που βρίσκουμε να συλλέγει τα δεδομένα του, όπως URL, τίτλος και περιεχόμενο.\n",
    "- Μετέπειτα η εντολή 'response = requests.get(url)' στέλνει ένα αίτημα GET στο URL και παίρνει πίσω το περιεχόμενο της ιστοσελίδας.\n",
    "- Με την βοήθεια της BeautifulSoup μπορούμε να αναλύσουμε (parse) και να επεξεργαστούμε HTML και XML έγγραφα. Έτσι, με το όρισμα response.content παίρνουμε το περιεχόμενο της σελίδας σε δυαδική μορφή και με το 'html.parser' δηλώνουμε τον αναλυτή (parser) που θέλουμε να χρησιμοποιήσει η BeautifulSoup για να είναι εύκολα επεξεργάσιμο το περιεχόμενο σε HTML.\n",
    "- Το αντικείμενο soup που δημιουργήσαμε βρίσκει το πρώτο h1 στοιχείο, το οποίο είναι συνήθως ο τίτλος της σελίδας και είναι μοναδικός και το αποθηκεύει στην μεταβλητή title.\n",
    "- Με την soup βρίσκουμε όλα τα στοιχεία p της σελίδας, τα οποία είναι οι παραγράφοι και τα αποθηκεύουμε στην μεταβλητή paragraphs.\n",
    "- Βάζουμε στην μεταβλητή content την ένωση όλων τις παραγράφων σε ένα ενιαίο string.\n",
    "- Επιστρέφουμε ένα λεξικό (dictionary) με τα δεδομένα του κάθε άρθρου, δηλαδή τον τίτλο, το URL και το περιεχόμενο.\n",
    "- Δημιουργούμε μια λίστα to_visit με τα URLs τα οποία θα επισκεφθούμε στην συνέχεια, ξεκινώντας από το start_url.\n",
    "- Χρησιμοποιούμε μια δομή επανάληψης while η οποία επαναλαμβάνεται όσο υπάρχουν διευθύνσεις που δεν έχουν επισκεφθεί και όσο δεν έχουμε φθάσει το αριθμητικό όριο άρθρων num_articles.\n",
    "- Παίρνουμε την πρώτη διεύθυνση από την λίστα to_visit.\n",
    "- Ελέγχουμε αν η διεύθυνση που θέλουμε να επισκεφθούμε current_url υπάρχει ήδη στο σύνολο visited και αν ναι την αγνοούμε.\n",
    "- Προσθέτουμε την διεύθυνση στο σύνολο visited για να δείξουμε ότι την έχουμε επισκεφθεί.\n",
    "- Αν βρεθούμε στην σελίδα \"Main Page\" της Wikipedia δεν συλλέγουμε τα δεδομένα της και την αγνοούμε.\n",
    "- Εκτυπώνεται το URL που ανακτάται εκείνη την στιγμή.\n",
    "- Καλείται η συνάρτηση fetch_article_data για να ανακτήσει τα δεδομένα από την τωρινή διεύθυνση current_url.\n",
    "- Αν τα δεδομένα που περιέχονται στην article είναι έγκυρα, προσθέτονται στην λίστα article.\n",
    "- Αναλύει ξανά το περιεχόμενο της ιστοσελίδας στην οποία βρισκόμαστε τώρα για να βρει κανούργιους συνδυασμούς.\n",
    "- Βρίσκουμε όλα τα στοιχεία a που περιέχουν το href, δηλαδή βρίσκουμε όλους τους συνδέσμους.\n",
    "- Ψάχνουμε μόνο τους συνδέσμους οι οποίοι ξεκινούν με '/wiki/', δηλαδή τα άρθρα, και δεν περιέχουν τον χαρακτήρα : έτσι ώστε να αποφύγουμε τις ειδικές σελίδες.\n",
    "- Προσθέτουμε το base_url και το href για να καταλήξουμε με μία πλήρη διεύθυνση full_url.\n",
    "- Ελέγχουμε αν αυτή η διεύθυνση full_url δεν υπάρχει ήδη στο σύνολο visited, δηλαδή αν δεν την έχουμε επισκεφθεί και αν ναι τότε την προσθέτουμε στο τέλος της λίστας to_visit.\n",
    "- Επιστρέφεται η λίστα articles με τα δεδομένα των αρθρών που συλλέξαμε."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Συνάρτηση για να ανακτήσουμε συγκεκριμένα άρθρα\n",
    "def fetch_articles(start_url, num_articles = 10):\n",
    "    base_url = \"https://en.wikipedia.org/\"\n",
    "    visited = set() #Σύνολο url τα οποία έχουμε ήδη επισκεφτεί\n",
    "    articles = [] #Δεδομένα των άρθρων\n",
    "    \n",
    "    #Συνάρτηση για να συλλέξουμε τα δεδομένα της κάθε σελίδας\n",
    "    def fetch_article_data(url):\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        title = soup.find('h1').text\n",
    "        paragraphs = soup.find_all('p')\n",
    "        content = \" \".join([p.text for p in paragraphs])\n",
    "        return {\"title\": title, \"url\": url, \"content\": content}\n",
    "    \n",
    "    #Τα url τα οποία θα επισκεφτούμε στην συνέχεια\n",
    "    to_visit = [start_url]\n",
    "    while to_visit and len(articles) < num_articles:\n",
    "        current_url = to_visit.pop(0)\n",
    "        if current_url in visited:\n",
    "            continue\n",
    "        visited.add(current_url)\n",
    "        \n",
    "        #Αποφεύγουμε το Main Page της Wikipedia\n",
    "        if 'Main_Page' in current_url:\n",
    "            continue\n",
    "        \n",
    "        print(f\"Fetching: {current_url}\")\n",
    "        article = fetch_article_data(current_url)\n",
    "        if article:\n",
    "            articles.append(article)\n",
    "        \n",
    "        response = requests.get(current_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for link in soup.find_all('a', href = True):\n",
    "            href = link['href']\n",
    "            if href.startswith('/wiki/') and ':' not in href:\n",
    "                full_url = base_url + href\n",
    "                if full_url not in visited:\n",
    "                    to_visit.append(full_url)\n",
    "        \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ορίζουμε την θεματολογία μας, δηλαδή το αρχικό URL από το οποίο θα ξεκινήσουμε την αναζήτηση σχετικών άρθρων.\n",
    "- Καλείται η συνάρτηση fetch_articles για να συλλέξει 10 άρθρα, ξεκινώντας από το start_url και τα αποτελέσματα αποθηκεύονται στην λίστα articles.\n",
    "- Με την εντολή open ανοίγει το αρχείο articles.json στο οποίο μπορούμε να γράψουμε και έχει ως κωδικοποίηση το utf-8.\n",
    "- Μέσα σε αυτό το αρχείο μορφής json αποθηκεύονται τα δεδομένα της λίστας articles.\n",
    "- Τέλος, τυπώνεται σχετικό μήνυμα επιτυχίας."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_url = \"https://en.wikipedia.org/wiki/Processor_(computing)\"\n",
    "articles = fetch_articles(start_url, num_articles = 10)\n",
    "\n",
    "with open(\"articles.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(articles, file, ensure_ascii=False, indent=4)\n",
    "print(\"The data has been saved to \\\"articles.json\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Βήμα 2. Προεπεξεργασία κειμένου (Text Processing):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Εισαγωγή των βιβλιοθηκών\n",
    "- Εισάγουμε την βιβλιοθήκη pandas γιατί την χρειαζόμαστε για την επεξεργασία των δεδομένων από αρχεία σε μορφή DataFrame.\n",
    "- Η βιβλιοθήκη nltk είναι απαραίτητη, καθώς παρέχει αρκετά εργαλεία για την επεξεργασία και την ανάλυση της φυσικής γλώσσας.\n",
    "- Η nltk.tokenize παρέχει την βιβλιοθήκη word_tokenize η οποία χρησιμοποιείται για να υλοποιήσει την διαδικασία του Tokenization, δηλαδή για να χωρίσει το κείμενο σε λέξεις ή αλλιώς tokens.\n",
    "- Η nltk.corpus παρέχει την stopwords, όπου σκοπός της είναι να μας παρέχει τις κοινές λέξεις που υπάρχουν σε ένα κείμενο, οι οποίες συνήθως δεν έχουν σημασιολογική βαρύτητα, όπως 'and', 'but', 'the'.\n",
    "- Η nltk.stem παρέχεις τις PorterStemmer και WordNetLemmatizer. Από την PorterStemmer χρησιμοποιώντας το Stemming μπορούμε για κάθε λέξη να αφαιρέσουμε την περιττή κατάληξη τους, πχ. το 'programs' γίνεται 'program'. Επίσης, από την WordNetLemmatizer χρησιμοποιώντας το Lemmatization παίρνουμε την κανονική μορφή μιας λέξης, πχ. το 'worse' γίνεται 'bad'.\n",
    "- Η βιβλιοθήκη string παρέχει διάφορους χαρακτήρες στίξης, την χρησιμοποιύμε έτσι ώστε να μπορέσουμε να αφαιρέσουμε από το κείμενο τα σημεία στίξης που υπάρχουν."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Με την χρήση του nltk.download κατεβάζουμε τους απαίτητους πόρους για να επεξεργαστούμε κατάλληλα το κείμενο όπως επιθυμούμε.\n",
    "- Το punkt χρειάζεται για το Tokenization.\n",
    "- Το stopwords μας παρέχει τις λίστες με τα stop words.\n",
    "- Το wordnet είναι απαραίτητο για το Lemmatization, διότι παρέχει την βάση δεδομένων των λεξικών που θα χρησιμοποιήσουμε."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Σκοπός της συνάρτησης process_content είναι να δέχεται ένα κείμενο και να το προετοιμάζει για παραπάνω ανάλυση στα επόμενα βήματα, αφαιρώντας πρώτα τους ειδικούς χαρακτήρες, τα stop words και να εφαρμόζει Lemmatization.\n",
    "- Αρχικά, για να αποφύγουμε σφάλματα ελέγχουμε αν η παράμετρος text είναι τύπου string.\n",
    "- Εφαρμόζουμε το Tokenization στο text και αρχικοποιούμε μια λίστα processed_words στην οποία θα αποθηκεύονται οι λέξεις που έχουν επεξεργαστεί.\n",
    "- Χρησιμοποιούμε μια δομή επανάληψης for με σκοπό να επεξεργαστούμε κάθε λέξη από την λίστα words.\n",
    "- Ελέγχουμε αν η λέξη είναι αριθμός και αν ναι τότε την προσθέτουμε κατευθείαν στην λίστα processed_words χωρίς καμία επεξεργασία.\n",
    "- Αν η λέξη είναι συμβολοσειρά τότε με την βοήθεια της string.punctuation, η οποία περιέχει όλους τους χαρακτήρες στίξης, τους αφαιρούμε από την αρχή ή το τέλος της λέξης με την χρήση της strip(). Στη συνέχεια, υλοποιούμε την διαδικασία του Lemmatization, δηλαδή με το WordNetLemmatizer μετατρέπουμε την λέξη στην βασική της μορφή. Επίσης, για την διαδικασία της αφαίρεσης των stop words ελέγχουμε αν η λέξη σε πεζά γράμματα δεν ανήκει στα στο σύνολο των stop words και αν όχι τότε την προσθέτουμε στην λίστα με τις επεξεργασμένες λέξεις.\n",
    "- Τέλος, χρησιμοποιούμε την join() για να ενώσουμε όλες τις επεξεργασμένες λέξεις με κενά ανάμεσα σε μία συμβολοσειρά και το επιστρέφουμε."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    # Tokenization\n",
    "    words = word_tokenize(text)\n",
    "    processed_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            processed_words.append(word)\n",
    "        else:\n",
    "            # Αφαίρεση ειδικών χαρακτήρων\n",
    "            word = word.strip(string.punctuation)\n",
    "            \n",
    "            # Lemmatization\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "            \n",
    "            # Αφαίρεση stop words\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            if word.lower() not in stop_words:\n",
    "                processed_words.append(word.lower())\n",
    "    \n",
    "    return ' '.join(processed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Διαβάζουμε το αρχείο articles.json και το μετρατρέπουμε σε DataFrame, εκτυπώνοντας σχετικό μήνυμα αν παρουσιαστεί σφάλμα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dataframe = pd.read_json(\"articles.json\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading JSON file: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Για να είμαστε σίγουροι ότι τα κείμενα μας έχουν επεξεργαστεί σωστά θα τα ελέγξουμε εφαρμόζοντας σε κάθε στήλη του DataFrame την process_content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in dataframe.columns:\n",
    "    dataframe[column] = dataframe[column].apply(process_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Μετονομάζουμε τις στήλες του DataFrame για να είναι πιο κατανοητές και φιλικές."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = {\n",
    "    dataframe.columns[0]: 'Title',\n",
    "    dataframe.columns[1]: 'Url',\n",
    "    dataframe.columns[2]: 'Content'\n",
    "}\n",
    "dataframe = dataframe.rename(columns=new_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αποθηκεύουμε το επεξεργασμένο πλέον DataFrame στο καινούργιο αρχείο processed.json και εκτυπώνουμε σχετικό μήνυμα επιτυχίας."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_json(\"processed.json\", orient='records', indent=4)\n",
    "\n",
    "print(\"The data has been saved to \\\"processed.json\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Βήμα 3. Ευρετήριο (indexing): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Εισάγουμε τις βιβλιοθήκες:\n",
    "- json για να φορτώσουμε τα δεδομένα του αρχείου processed.json που δημιουργήσαμε στο προηγούμενο βήμα και\n",
    "- την defaultdict από την collections έτσι ώστε  να δημιουργήσουμε ένα λεξικό το οποίο θα  περιέχει λίστες αυτόματα χωρίς να χρειάζεται να το ελέγχουμε εμείς."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ανοίγουμε το αρχείο processed.json σε λειτουργία ανάγνωσης ('r'), διαβάζουμε το περιεχόμενο και το μετατρέπουμε σε ένα λεξικό με μορφή λίστας, το οποίο ποθηκεύουμε στην μεταβλητή documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed.json', 'r') as json_file:\n",
    "    documents = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Δημιουργούμε ένα αντίστροφο ευρετήριο, δηλαδή ένα λεξικό."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = defaultdict(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Χρησιμοποιούμε μία δομή επανάληψης for η οποία τρέχει για κάθε έγγραφο της λίστας documents.\n",
    "- Χρησιμοποιούμε άλλη μία for για κάθε πεδίο των εγγράφων, με την βοήθεια της document.items() επιστρέφονται τα πεδία που υπάρχουν με τις τιμές τους, π.χ. Title.\n",
    "- Ελέγχει αν το field_value είναι συμβολοσειρά και αν ναι τότε αυτό το πεδίο χωρίζεται σε ξεχωριστές λέξεις με την split() και αποθηκεύεται στην λίστα terms. Αυτό το κάνουμε με σκοπό να μην συμπεριλάβουμε τα πεδία που δεν περιέχουν σημασιολογικό κείμενο όπως το URL.\n",
    "- Ακόμη, τρέχουμε ακόμη μία for για να διατρέξουμε την κάθε λέξη μέσα στην λίστα terms. Ελέγχουμε αν το doc_id υπάρχει ήδη στο inverted_index για τον συγκεκριμένο όρο και αν όχι τότε το προσθέτουμε στο ευρετήριο. Σκοπός αυτού του βήματος είναι να αποφύγουμε την καινούργια εγγραφή μιας ήδη υπάρχουσας λέξης στο ίδιο έγγραφο."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_id, document in enumerate(documents):\n",
    "    for field, field_value in document.items():\n",
    "        if isinstance(field_value, str):\n",
    "            terms = field_value.split()\n",
    "            \n",
    "            for term in terms:\n",
    "                if doc_id not in inverted_index[term]:\n",
    "                    inverted_index[term].append(doc_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Το αρχείο inverted_index.json ανοίγει σε λειτουργία εγγραφής ('w') και αποθηκεύει το inverted_index μέσα σε αυτό. Τέλος, ευμφανίζεται μήνυμα επιτυχίας προς τον χρήστη."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inverted_index.json', 'w') as json_file:\n",
    "    json.dump(inverted_index, json_file, indent=4)\n",
    "\n",
    "print(\"The data has been saved to \\\"inverted_index.json\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Βήμα 4. Μηχανή αναζήτησης (Search Engine):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " α) Επεξεργασία ερωτήματος (Query Processing): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αρχικά εισάγουμε την βιβλιοθήκη json, γιατί θα την χρειαστούμε για να φορτώσουμε και να χρησιμοποιήσουμε στην αναζήτηση το αρχείο inverted_index.json από το προηγούμενο ερώτημα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Εδώ η συνάρτηση load_inverted_index χρηιμοποιείται για να φορτώσουμε το ανεστραμμένο ευρετήριο από το αρχείο inverted_index.json. Με την open() διαβάζουμε το αρχείο και με την json.load(f) φορτώνουμε το περιεχόμενο του επιστρέφοντας το σε μορφή λεξικού."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_inverted_index(filepath):\n",
    "    with open(filepath, \"r\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Σκοπός της συνάρτησης boolean_search είναι να γίνεται αναζήτηση των όρων σε όλα τα κείμενα με την χρήση των λογικών τελεστών AND, OR και NOT.\n",
    "- Ξεκινώντας, μετατρέπουμε το ερώτημα που θα μας δώσει ο χρήστης σε πεζά γράμματα και διαχωρίζουμε τις λέξεις με κενά βάζοντας το αποτέλεσμα στο terms.\n",
    "- Αν ο χρήστης δεν εισάγει τίποτα τότε η συνάρτηση επιστρέφει ένα κενό σύνολο.\n",
    "- Στη συνέχεια, με την χρήση get() προσπαθούμε να βρούμε ποια έγγραφα περιέχουν το συγκεκριμένο στοιχείο ξεκινώντας από το πρώτο και το αποθηκεύουμε στο result_set. Αν δεν υπάρχει στο ευρετήριο τότε αποθηκεύεται μια κενή λίστα.\n",
    "- Μετά ελέγχουμε τους υπόλοιπους όρους και τους τελεστές που ζήτησε ο χρήστης.\n",
    "- Αν υπάρχει τελεστής στο ερώτημα αλλά δεν υπάρχει όρος μετά από αυτόν, εμφανίζεται σχετικό μήνυμα σφάλματος.\n",
    "- Έπειτα, κάνουμε την ίδια διαδικασία πάλι για τον επόμενο όρο της αναζήτησης και βρίσκουμε το έγγραφα που τον περιέχουν.\n",
    "- Με την δομή επιλογής if αν ο χρήστης επέλεξε τον τελεστή and τότε με την χρήση του result_set.intersection() κρατάμε μόνο τα κοινά έγγραφα στα οποία περιέχονται οι όροι, στην επιλογή or με την result_set.union() ενώνουμε τα έγγραφα που περιέχουν τουλάχιστον έναν από τους όρους, στην επιλογή not με την result_set.difference() αφαιρούμε τα έγγραφα που δεν περιέχουν τους όρους και σε κάθε άλλη περίπτωση εμφανίζεται σχετικό μήνυμα λάθους.\n",
    "- Η επανάληψη συνεχίζεται προχωρώντας το index i κατά δύο θέσεις για να εκτελέσουμε την διαδικασία για τον επόμενο τελεστή.\n",
    "- Τέλος, επιστρέφουμε τα αποτελέσματα της αναζήτησης με το σύνολο result_set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_search(query, inverted_index):\n",
    "    terms = query.lower().split()\n",
    "    if not terms:\n",
    "        return None, \"Το ερώτημα είναι κενό.\"\n",
    "\n",
    "    if len(terms) % 2 == 0:\n",
    "        return None, f\"Λάθος: Λείπει όρος μετά τον τελεστή '{terms[-1]}'.\"\n",
    "\n",
    "    result_set = set(inverted_index.get(terms[0], []))\n",
    "    \n",
    "    i = 1\n",
    "    while i < len(terms):\n",
    "        operator = terms[i]  #AND, OR, NOT\n",
    "        \n",
    "        next_term = terms[i + 1]\n",
    "        next_docs = set(inverted_index.get(next_term, []))\n",
    "        \n",
    "        # Εκτέλεση της λειτουργίας\n",
    "        if operator == \"and\":\n",
    "            result_set = result_set.intersection(next_docs)\n",
    "        elif operator == \"or\":\n",
    "            result_set = result_set.union(next_docs)\n",
    "        elif operator == \"not\":\n",
    "            result_set = result_set.difference(next_docs)\n",
    "        else:\n",
    "            return None, f\"Λάθος: Άγνωστος τελεστής '{operator}'.\"\n",
    "        \n",
    "        i += 2\n",
    "    \n",
    "    return result_set, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Βρισκόμαστε στην main, η οποία διαχειρίζεται την διαδικασία της αναζήτησης.\n",
    "- Σε αυτήν φορτώνουμε το ανεστραμμένο ευρετήριο inverted_index.json και το αποθηκεύουμε στο filepath, καθώς θα χρειαστεί σας παράμετρο για την συνάρτηση load_inverted_index.\n",
    "- Εκτυπώνουμε βασικά μηνύματα καλωσορίσματος φιλικά προς τον χρήστη και του εξηγούμε πως να χρησιμοποιήσει την αναζήτηση.\n",
    "- Με την χρήση ενός ατελείωτου βρόχου ζητάμε να εισάγει το ερώτημα του το οποίο αποθηκεύεται στο query. Για να σταματήσει την αναζήτηση πληκτρολογεί exit.\n",
    "- Εκτελούμε την αναζήτηση με την boolean_search και αποθηκεύουμε τα αποτελέσματα στο results.\n",
    "- Τέλος, αν υπάρχει error τότε τερματίζει το πρόγραμμα, αν βρεθούν τα αποτελέσματα τα εμφανίζουμε, αλλιώς εμφανίζεται σχετικό μήνυμα για την περίπτωη που δεν βρέθηκαν αποτελέσματα για το ερώτημα του χρήστη."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    filepath = \"inverted_index.json\"  \n",
    "    inverted_index = load_inverted_index(filepath)\n",
    "\n",
    "    print(\"Καλωσήρθατε στη Μηχανή Αναζήτησης!\")\n",
    "    print(\"Πληκτρολογήστε το ερώτημά σας χρησιμοποιώντας Boolean τελεστές (AND, OR, NOT) ή 'exit' για έξοδο.\")\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"\\nΕρώτημα: \").strip()\n",
    "        \n",
    "        if query.lower() == \"exit\":\n",
    "            print(\"Έξοδος από τη Μηχανή Αναζήτησης!\")\n",
    "            break\n",
    "        \n",
    "        results, error = boolean_search(query, inverted_index)\n",
    "        \n",
    "        if error:\n",
    "            print(error)\n",
    "        elif results:\n",
    "            print(f\"Αποτελέσματα για το ερώτημα '{query}': {results}\")\n",
    "        else:\n",
    "            print(f\"Δεν βρέθηκαν αποτελέσματα για το ερώτημα '{query}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Διασφαλίζουμε ότι η main θα εκτελεστεί μόνο όταν το αρχείο τρέχει απευθείας."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "β) Κατάταξη αποτελεσμάτων (Ranking):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Σκοπός αυτού του προγράμματος είναι να παρέχει στον χρήστη τρεις τρόπους αναζήτησης: Boolean Search, TF-IDF Ranking και BM25 Ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Για να τρέξει το πρόγραμμα πρέπει να εγκατασταθεί η βιβλιοθήκη rank-bm25 με την παρακάτω εντολή."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install rank-bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Γίνεται εισαγωγή των βιβλιοθηκών που χρειαζόμαστε:\n",
    "- Η TfidfVectorizer από την sklearn.feature_extraction.text είναι χρήσιμη για τον αλγόριθμο TF-IDF, καθώς μπορεί να μετατρέψει τα κείμενα σε αριθμητικά διανύσματα βάσει του πόσο συχνά εμφανίζονται οι λέξεις.\n",
    "- Χρησιμοποιούμε την cosine_similarity από την sklearn.metrics.pairwise πάλι για τον αλγόριθμο TF-IDF. Μπορεί να μας παρέχει με την ομοιότητα δύο διανυσμάτων.\n",
    "- Η BM25Okapi από την rank_bm25 είναι αναγκαία για την υλοποίηση του αλγορίθμου BM25.\n",
    "- Η json χρειάζεται για την φόρτωση αρχείων μορφής json.\n",
    "- Η numpy έχει διάφορες μαθηματικές λειτουργίας, όπως η ταξινόμηση των εγγράφων με βάση τα σκορ που μας είναι χρήσιμη στο συγκεκριμένο πρόγραμμα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Σκοπός της παρακάτω συνάρτησης είναι να γίνει η φόρτωση των εγγράφων από το αρχείο μορφής json που περιέχεται στο filepath.\n",
    "- Ανοίγουμε το αρχείο json, διαβάζουμε τα δεδομένα του και τα βάζουμε στην μεταβλητή data.\n",
    "- Αν η data περιέχει λίστα, τότε επιστρέφουμε μόνο το πεδίο του content.\n",
    "- Αν περιέχει λεξικό το επιστρέφουμε όπως είναι.\n",
    "- Αν έχει κάποια διαφορετική δομή, τότε εμφανίζεται μήνυμα λάθους."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "        if isinstance(data, list):\n",
    "            return [doc[\"content\"] for doc in data if \"content\" in doc]\n",
    "        elif isinstance(data, dict):\n",
    "            return data\n",
    "        else:\n",
    "            raise ValueError(\"Το αρχείο δεν έχει υποστηριζόμενη μορφή.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boolean Search\n",
    "def boolean_search(query, inverted_index):\n",
    "    terms = query.lower().split()\n",
    "    if not terms:\n",
    "        return set()\n",
    "\n",
    "    result_set = set(inverted_index.get(terms[0], []))\n",
    "    i = 1\n",
    "    while i < len(terms):\n",
    "        operator = terms[i]\n",
    "        if i + 1 >= len(terms):\n",
    "            break\n",
    "        next_term = terms[i + 1]\n",
    "        next_docs = set(inverted_index.get(next_term, []))\n",
    "        if operator == \"and\":\n",
    "            result_set = result_set.intersection(next_docs)\n",
    "        elif operator == \"or\":\n",
    "            result_set = result_set.union(next_docs)\n",
    "        elif operator == \"not\":\n",
    "            result_set = result_set.difference(next_docs)\n",
    "        i += 2\n",
    "    return result_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Συνάρτηση για τη φόρτωση εγγράφων από ένα αρχείο JSON\n",
    "def load_documents(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "        # Αν το JSON περιέχει λίστα, επιστρέφουμε τα περιεχόμενα του πεδίου \"content\"\n",
    "        if isinstance(data, list):\n",
    "            return [doc[\"content\"] for doc in data if \"content\" in doc]\n",
    "        # Αν το JSON περιέχει λεξικό (π.χ. το ανεστραμμένο ευρετήριο), επιστρέφουμε το λεξικό\n",
    "        elif isinstance(data, dict):\n",
    "            return data\n",
    "        # Αν δεν είναι καμία από τις δύο μορφές, εμφανίζεται μήνυμα λάθους\n",
    "        else:\n",
    "            raise ValueError(\"Το αρχείο δεν έχει υποστηριζόμενη μορφή.\")\n",
    "\n",
    "# Boolean Search για αναζήτηση εγγράφων με βάση Boolean λογική (AND, OR, NOT)\n",
    "def boolean_search(query, inverted_index):\n",
    "    terms = query.lower().split()  # Διαχωρισμός του ερωτήματος σε λέξεις\n",
    "    if not terms:  # Αν το ερώτημα είναι κενό, επιστρέφουμε κενό σύνολο\n",
    "        return set()\n",
    "\n",
    "    # Ξεκινάμε με τα έγγραφα που περιέχουν την πρώτη λέξη\n",
    "    result_set = set(inverted_index.get(terms[0], []))\n",
    "    i = 1\n",
    "    while i < len(terms):  # Επεξεργασία των υπόλοιπων λέξεων και τελεστών\n",
    "        operator = terms[i]  # Παίρνουμε τον Boolean τελεστή (AND, OR, NOT)\n",
    "        if i + 1 >= len(terms):  # Αν δεν υπάρχει άλλη λέξη μετά τον τελεστή, σταματάμε\n",
    "            break\n",
    "        next_term = terms[i + 1]  # Επόμενη λέξη\n",
    "        next_docs = set(inverted_index.get(next_term, []))  # Έγγραφα που περιέχουν τη λέξη\n",
    "        # Εκτέλεση της αντίστοιχης Boolean λειτουργίας\n",
    "        if operator == \"and\":\n",
    "            result_set = result_set.intersection(next_docs)\n",
    "        elif operator == \"or\":\n",
    "            result_set = result_set.union(next_docs)\n",
    "        elif operator == \"not\":\n",
    "            result_set = result_set.difference(next_docs)\n",
    "        i += 2  # Προχωράμε στον επόμενο τελεστή/λέξη\n",
    "    return result_set\n",
    "\n",
    "# TF-IDF Ranking για κατάταξη εγγράφων με βάση τη σχετικότητα (TF-IDF score)\n",
    "def tfidf_ranking(query, documents):\n",
    "    vectorizer = TfidfVectorizer()  # Δημιουργία του TF-IDF Vectorizer\n",
    "    doc_vectors = vectorizer.fit_transform(documents)  # Υπολογισμός TF-IDF για τα έγγραφα\n",
    "    query_vector = vectorizer.transform([query.lower()])  # Μετατροπή του ερωτήματος σε διάνυσμα\n",
    "    scores = cosine_similarity(query_vector, doc_vectors).flatten()  # Υπολογισμός cosine similarity\n",
    "    sorted_indices = np.argsort(scores)[::-1]  # Ταξινόμηση των εγγράφων με βάση τη βαθμολογία\n",
    "    # Επιστροφή μόνο των εγγράφων με βαθμολογία > 0\n",
    "    return [(idx, scores[idx]) for idx in sorted_indices if scores[idx] > 0.0]\n",
    "\n",
    "# BM25 Ranking για κατάταξη εγγράφων με βάση το BM25\n",
    "def bm25_ranking(query, documents):\n",
    "    tokenized_docs = [doc.split() for doc in documents]  # Διαχωρισμός των εγγράφων σε λέξεις\n",
    "    bm25 = BM25Okapi(tokenized_docs)  # Δημιουργία BM25 μοντέλου\n",
    "    tokenized_query = query.split()  # Διαχωρισμός του ερωτήματος σε λέξεις\n",
    "    scores = bm25.get_scores(tokenized_query)  # Υπολογισμός BM25 scores\n",
    "    sorted_indices = np.argsort(scores)[::-1]  # Ταξινόμηση των εγγράφων με βάση τη βαθμολογία\n",
    "    # Επιστροφή των εγγράφων με βαθμολογίες\n",
    "    return [(idx, scores[idx]) for idx in sorted_indices if scores[idx] > 0.0]\n",
    "\n",
    "# Κύρια συνάρτηση για την εκτέλεση της μηχανής αναζήτησης\n",
    "def main():\n",
    "    # Αρχεία εισόδου\n",
    "    filepath_docs = \"articles.json\"  # Αρχείο με τα έγγραφα\n",
    "    filepath_index = \"inverted_index.json\"  # Αρχείο με το ανεστραμμένο ευρετήριο\n",
    "    documents = load_documents(filepath_docs)  # Φόρτωση των εγγράφων\n",
    "    inverted_index = load_documents(filepath_index)  # Φόρτωση του ανεστραμμένου ευρετηρίου\n",
    "\n",
    "    # Εμφάνιση επιλογών στον χρήστη\n",
    "    print(\"Επιλέξτε αλγόριθμο ανάκτησης:\")\n",
    "    print(\"1. Boolean Retrieval\")\n",
    "    print(\"2. TF-IDF Ranking\")\n",
    "    print(\"3. BM25 Ranking\")\n",
    "    print(\"Πληκτρολογήστε 'exit' για έξοδο.\")\n",
    "\n",
    "    while True:\n",
    "        choice = input(\"\\nΕπιλογή: \").strip()  # Είσοδος επιλογής από τον χρήστη\n",
    "        if choice.lower() == \"exit\":  # Έξοδος από το πρόγραμμα\n",
    "            print(\"Έξοδος από τη Μηχανή Αναζήτησης!\")\n",
    "            break\n",
    "\n",
    "        query = input(\"Ερώτημα: \").strip()  # Είσοδος ερωτήματος\n",
    "        if not query:  # Αν το ερώτημα είναι κενό\n",
    "            print(\"Το ερώτημα δεν μπορεί να είναι κενό.\")\n",
    "            continue\n",
    "\n",
    "        if choice == \"1\":  # Boolean Retrieval\n",
    "            results = boolean_search(query, inverted_index)\n",
    "            if results:\n",
    "                print(\"\\nBoolean Results:\")\n",
    "                for idx in results:\n",
    "                    if idx < len(documents):  # Έλεγχος αν το έγγραφο υπάρχει\n",
    "                        print(f\"Έγγραφο {idx}\")\n",
    "                    else:\n",
    "                        print(f\"Το έγγραφο {idx} δεν υπάρχει στο articles.json.\")\n",
    "            else:\n",
    "                print(\"Δεν βρέθηκαν αποτελέσματα.\")\n",
    "\n",
    "        elif choice == \"2\":  # TF-IDF Ranking\n",
    "            results = tfidf_ranking(query, documents)\n",
    "            print(\"\\nTF-IDF Results:\")\n",
    "            for idx, score in results:\n",
    "                if idx < len(documents):  # Έλεγχος αν το έγγραφο υπάρχει\n",
    "                    print(f\"Έγγραφο {idx} (Σκορ: {score:.4f})\")\n",
    "                else:\n",
    "                    print(f\"Το έγγραφο {idx} δεν υπάρχει στο articles.json.\")\n",
    "\n",
    "        elif choice == \"3\":  # BM25 Ranking\n",
    "            results = bm25_ranking(query, documents)\n",
    "            print(\"\\nBM25 Results:\")\n",
    "            for idx, score in results:\n",
    "                if idx < len(documents):  # Έλεγχος αν το έγγραφο υπάρχει\n",
    "                    print(f\"Έγγραφο {idx} (Σκορ: {score:.4f})\")\n",
    "                else:\n",
    "                    print(f\"Το έγγραφο {idx} δεν υπάρχει στο articles.json.\")\n",
    "\n",
    "        else:  # Μη έγκυρη επιλογή\n",
    "            print(\"Μη έγκυρη επιλογή. Παρακαλώ προσπαθήστε ξανά.\")\n",
    "\n",
    "# Εκκίνηση προγράμματος\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
