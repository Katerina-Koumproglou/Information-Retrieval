{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Βήμα 1. Συλλογή δεδομένων:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://en.wikipedia.org/wiki/Processor_(computing)\n",
      "Fetching: https://en.wikipedia.org//wiki/Processor_(computing)\n",
      "Fetching: https://en.wikipedia.org//wiki/Processor_(disambiguation)#Computing\n",
      "Fetching: https://en.wikipedia.org//wiki/Computing\n",
      "Fetching: https://en.wikipedia.org//wiki/Computer_science\n",
      "Fetching: https://en.wikipedia.org//wiki/Circuit_(computer_science)\n",
      "Fetching: https://en.wikipedia.org//wiki/Memory_(computing)\n",
      "Fetching: https://en.wikipedia.org//wiki/Microprocessor\n",
      "Fetching: https://en.wikipedia.org//wiki/Metal%E2%80%93oxide%E2%80%93semiconductor\n",
      "The data has been saved to \"articles.json\"\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "#Συνάρτηση για να ανακτήσουμε συγκεκριμένα άρθρα\n",
    "def fetch_articles(start_url, num_articles = 10):\n",
    "    base_url = \"https://en.wikipedia.org/\"\n",
    "    visited = set() #Σύνολο url τα οποία έχουμε ήδη επισκεφτεί\n",
    "    articles = [] #Δεδομένα των άρθρων\n",
    "    \n",
    "    #Συνάρτηση για να συλλέξουμε τα δεδομένα της κάθε σελίδας\n",
    "    def fetch_article_data(url):\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        title = soup.find('h1').text\n",
    "        paragraphs = soup.find_all('p')\n",
    "        content = \" \".join([p.text for p in paragraphs])\n",
    "        return {\"title\": title, \"url\": url, \"content\": content}\n",
    "    \n",
    "    #Τα url τα οποία θα επισκεφτούμε στην συνέχεια\n",
    "    to_visit = [start_url]\n",
    "    while to_visit and len(articles) < num_articles:\n",
    "        current_url = to_visit.pop(0)\n",
    "        if current_url in visited:\n",
    "            continue\n",
    "        visited.add(current_url)\n",
    "        \n",
    "        #Αποφεύγουμε το Main Page της Wikipedia\n",
    "        if 'Main_Page' in current_url:\n",
    "            continue\n",
    "        \n",
    "        print(f\"Fetching: {current_url}\")\n",
    "        article = fetch_article_data(current_url)\n",
    "        if article:\n",
    "            articles.append(article)\n",
    "        \n",
    "        response = requests.get(current_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for link in soup.find_all('a', href = True):\n",
    "            href = link['href']\n",
    "            if href.startswith('/wiki/') and ':' not in href:\n",
    "                full_url = base_url + href\n",
    "                if full_url not in visited:\n",
    "                    to_visit.append(full_url)\n",
    "        \n",
    "    return articles\n",
    "        \n",
    "\n",
    "start_url = \"https://en.wikipedia.org/wiki/Processor_(computing)\"\n",
    "articles = fetch_articles(start_url, num_articles = 9)\n",
    "\n",
    "with open(\"articles.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(articles, file, ensure_ascii=False, indent=4)\n",
    "print(\"The data has been saved to \\\"articles.json\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Βήμα 2. Προεπεξεργασία κειμένου (Text Processing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Μαρία\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Μαρία\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Μαρία\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has been saved to \"processed.json\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Συνάρτηση για επεξεργασία κειμένου\n",
    "def process_content(text):\n",
    "    if not isinstance(text, str):  # Έλεγχος ότι η είσοδος είναι string\n",
    "        return text\n",
    "    \n",
    "    # Tokenization\n",
    "    words = word_tokenize(text)\n",
    "    processed_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word.isdigit():  # Αν είναι αριθμός, το προσθέτουμε απευθείας\n",
    "            processed_words.append(word)\n",
    "        else:\n",
    "            # Αφαίρεση ειδικών χαρακτήρων\n",
    "            word = word.strip(string.punctuation)\n",
    "            \n",
    "            # Lemmatization\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "            \n",
    "            # Αφαίρεση stop words\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            if word.lower() not in stop_words:\n",
    "                processed_words.append(word.lower())\n",
    "    \n",
    "    return ' '.join(processed_words)\n",
    "\n",
    "# Ανάγνωση δεδομένων από αρχείο JSON\n",
    "try:\n",
    "    dataframe = pd.read_json(\"articles.json\")  # διαδρομή αρχείου\n",
    "except Exception as e:\n",
    "    print(f\"Error reading JSON file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Επεξεργασία δεδομένων DataFrame\n",
    "for column in dataframe.columns:\n",
    "    dataframe[column] = dataframe[column].apply(process_content)\n",
    "\n",
    "# Μετονομασία στηλών\n",
    "new_columns = {\n",
    "    dataframe.columns[0]: 'Title',\n",
    "    dataframe.columns[1]: 'Url',\n",
    "    dataframe.columns[2]: 'Content'\n",
    "}\n",
    "dataframe = dataframe.rename(columns=new_columns)\n",
    "\n",
    "# Αποθήκευση επεξεργασμένων δεδομένων σε νέο αρχείο JSON\n",
    "dataframe.to_json(\"processed.json\", orient='records', indent=4)\n",
    "\n",
    "print(\"The data has been saved to \\\"processed.json\\\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Βήμα 3. Ευρετήριο (indexing): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has been saved to \"inverted_index.json\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Διαβάζουμε τα επεξεργασμένα δεδομένα από το αρχείο processed.json\n",
    "with open('processed.json', 'r') as json_file:\n",
    "    documents = json.load(json_file)  # Φορτώνουμε το JSON σε μορφή λίστας/λεξικού\n",
    "\n",
    "# Αντίστροφο ευρετήριο (inverted index)\n",
    "inverted_index = defaultdict(list)  # Χρησιμοποιούμε defaultdict για να αποθηκεύσουμε τα IDs των εγγράφων\n",
    "\n",
    "# Επεξεργασία κάθε έγγραφου\n",
    "for doc_id, document in enumerate(documents):\n",
    "    # Σπλιτάρω και αποθηκεύω στο terms όλα τα κομμάτια\n",
    "    for field, field_value in document.items():\n",
    "        if isinstance(field_value, str):  # Ελέγχω αν το πεδίο είναι string\n",
    "            terms = field_value.split()  # Χωρίζω το κείμενο σε όρους\n",
    "            \n",
    "            # Δημιουργία του αντεστραμμένου ευρετηρίου (αντί για αριθμό εμφανίσεων, αποθηκεύουμε τα doc_id)\n",
    "            for term in terms:\n",
    "                if doc_id not in inverted_index[term]:  # Ελέγχουμε αν το doc_id είναι ήδη στη λίστα\n",
    "                    inverted_index[term].append(doc_id)\n",
    "\n",
    "# Αποθήκευση του αντεστραμμένου ευρετηρίου σε αρχείο JSON\n",
    "with open('inverted_index.json', 'w') as json_file:\n",
    "    json.dump(inverted_index, json_file, indent=4)  \n",
    "\n",
    "\n",
    "print(\"The data has been saved to \\\"inverted_index.json\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Βήμα 4. Μηχανή αναζήτησης (Search Engine):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
