{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Βήμα 1. Συλλογή δεδομένων:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Αρχικά, πρέπει να συμπεριλάβουμε στον κώδικα μας την βιβλιοθήκη requests έτσι ώστε να κάνουμε αιτήματρα HTTP.\n",
    "- Εισάγουμε από την βιβλιοθήκη bs4 το BeautifulSoup, το οποίο χρησιμοποιούμε για να αναλύσουμε το HTML μιας ιστοσελίδας, έτσι ώστε να εξάγουμε διάφορα δεδομένα που χρειαζόμαστε, όπως τίτλους και παραγράφους.\n",
    "- Ακόμη, εισάγουμε την βιβλιοθήκη json, η οποία χρησιμοποιείται για την διαχείριση δεδομένων σε μορφή JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Δημιουργούμε την συνάρτηση fetch_articles, σκοπός τη οποίας είναι να ανακτά τα δεδομένα των αρθρών από την ιστοσελίδα που θα ορίσουμε, ξεκινώντας από το start_url και μέχρι τον αριθμό των άρθρων που θέλουμε να συλλέξουμε num_articles, όπου στην συγκεκριμένη περίπτωση είναι 10 άρθρα.\n",
    "- Ορίζουμε την βάση διευθύνσεων από την οποία θα ξεκινήσουμε, δηλαδή την Wikipedia.\n",
    "- Στο σύνολο visited αποθηκεύουμε τα URLs που έχουμε ήδη επισκεφθεί, έτσι ώστε να μην τα επισκεφτούμε ξανά.\n",
    "- Δημιουργούμε την λίστα articles η οποία περιέχει τα δεδομένα των αρθρών που συλλέξαμε.\n",
    "- Σκοπός της συνάρτησης fetch_article_data είναι για κάθε άρθρο που βρίσκουμε να συλλέγει τα δεδομένα του, όπως URL, τίτλος και περιεχόμενο.\n",
    "- Μετέπειτα η εντολή 'response = requests.get(url)' στέλνει ένα αίτημα GET στο URL και παίρνει πίσω το περιεχόμενο της ιστοσελίδας.\n",
    "- Με την βοήθεια της BeautifulSoup μπορούμε να αναλύσουμε (parse) και να επεξεργαστούμε HTML και XML έγγραφα. Έτσι, με το όρισμα response.content παίρνουμε το περιεχόμενο της σελίδας σε δυαδική μορφή και με το 'html.parser' δηλώνουμε τον αναλυτή (parser) που θέλουμε να χρησιμοποιήσει η BeautifulSoup για να είναι εύκολα επεξεργάσιμο το περιεχόμενο σε HTML.\n",
    "- Το αντικείμενο soup που δημιουργήσαμε βρίσκει το πρώτο h1 στοιχείο, το οποίο είναι συνήθως ο τίτλος της σελίδας και είναι μοναδικός και το αποθηκεύει στην μεταβλητή title.\n",
    "- Με την soup βρίσκουμε όλα τα στοιχεία p της σελίδας, τα οποία είναι οι παραγράφοι και τα αποθηκεύουμε στην μεταβλητή paragraphs.\n",
    "- Βάζουμε στην μεταβλητή content την ένωση όλων τις παραγράφων σε ένα ενιαίο string.\n",
    "- Επιστρέφουμε ένα λεξικό (dictionary) με τα δεδομένα του κάθε άρθρου, δηλαδή τον τίτλο, το URL και το περιεχόμενο.\n",
    "- Δημιουργούμε μια λίστα to_visit με τα URLs τα οποία θα επισκεφθούμε στην συνέχεια, ξεκινώντας από το start_url.\n",
    "- Χρησιμοποιούμε μια δομή επανάληψης while η οποία επαναλαμβάνεται όσο υπάρχουν διευθύνσεις που δεν έχουν επισκεφθεί και όσο δεν έχουμε φθάσει το αριθμητικό όριο άρθρων num_articles.\n",
    "- Παίρνουμε την πρώτη διεύθυνση από την λίστα to_visit.\n",
    "- Ελέγχουμε αν η διεύθυνση που θέλουμε να επισκεφθούμε current_url υπάρχει ήδη στο σύνολο visited και αν ναι την αγνοούμε.\n",
    "- Προσθέτουμε την διεύθυνση στο σύνολο visited για να δείξουμε ότι την έχουμε επισκεφθεί.\n",
    "- Αν βρεθούμε στην σελίδα \"Main Page\" της Wikipedia δεν συλλέγουμε τα δεδομένα της και την αγνοούμε.\n",
    "- Εκτυπώνεται το URL που ανακτάται εκείνη την στιγμή.\n",
    "- Καλείται η συνάρτηση fetch_article_data για να ανακτήσει τα δεδομένα από την τωρινή διεύθυνση current_url.\n",
    "- Αν τα δεδομένα που περιέχονται στην article είναι έγκυρα, προσθέτονται στην λίστα article.\n",
    "- Αναλύει ξανά το περιεχόμενο της ιστοσελίδας στην οποία βρισκόμαστε τώρα για να βρει κανούργιους συνδυασμούς.\n",
    "- Βρίσκουμε όλα τα στοιχεία a που περιέχουν το href, δηλαδή βρίσκουμε όλους τους συνδέσμους.\n",
    "- Ψάχνουμε μόνο τους συνδέσμους οι οποίοι ξεκινούν με '/wiki/', δηλαδή τα άρθρα, και δεν περιέχουν τον χαρακτήρα : έτσι ώστε να αποφύγουμε τις ειδικές σελίδες.\n",
    "- Προσθέτουμε το base_url και το href για να καταλήξουμε με μία πλήρη διεύθυνση full_url.\n",
    "- Ελέγχουμε αν αυτή η διεύθυνση full_url δεν υπάρχει ήδη στο σύνολο visited, δηλαδή αν δεν την έχουμε επισκεφθεί και αν ναι τότε την προσθέτουμε στο τέλος της λίστας to_visit.\n",
    "- Επιστρέφεται η λίστα articles με τα δεδομένα των αρθρών που συλλέξαμε."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Συνάρτηση για να ανακτήσουμε συγκεκριμένα άρθρα\n",
    "def fetch_articles(start_url, num_articles = 10):\n",
    "    base_url = \"https://en.wikipedia.org/\"\n",
    "    visited = set() #Σύνολο url τα οποία έχουμε ήδη επισκεφτεί\n",
    "    articles = [] #Δεδομένα των άρθρων\n",
    "    \n",
    "    #Συνάρτηση για να συλλέξουμε τα δεδομένα της κάθε σελίδας\n",
    "    def fetch_article_data(url):\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        title = soup.find('h1').text\n",
    "        paragraphs = soup.find_all('p')\n",
    "        content = \" \".join([p.text for p in paragraphs])\n",
    "        return {\"title\": title, \"url\": url, \"content\": content}\n",
    "    \n",
    "    #Τα url τα οποία θα επισκεφτούμε στην συνέχεια\n",
    "    to_visit = [start_url]\n",
    "    while to_visit and len(articles) < num_articles:\n",
    "        current_url = to_visit.pop(0)\n",
    "        if current_url in visited:\n",
    "            continue\n",
    "        visited.add(current_url)\n",
    "        \n",
    "        #Αποφεύγουμε το Main Page της Wikipedia\n",
    "        if 'Main_Page' in current_url:\n",
    "            continue\n",
    "        \n",
    "        print(f\"Fetching: {current_url}\")\n",
    "        article = fetch_article_data(current_url)\n",
    "        if article:\n",
    "            articles.append(article)\n",
    "        \n",
    "        response = requests.get(current_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for link in soup.find_all('a', href = True):\n",
    "            href = link['href']\n",
    "            if href.startswith('/wiki/') and ':' not in href:\n",
    "                full_url = base_url + href\n",
    "                if full_url not in visited:\n",
    "                    to_visit.append(full_url)\n",
    "        \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ορίζουμε την θεματολογία μας, δηλαδή το αρχικό URL από το οποίο θα ξεκινήσουμε την αναζήτηση σχετικών άρθρων.\n",
    "- Καλείται η συνάρτηση fetch_articles για να συλλέξει 10 άρθρα, ξεκινώντας από το start_url και τα αποτελέσματα αποθηκεύονται στην λίστα articles.\n",
    "- Με την εντολή open ανοίγει το αρχείο articles.json στο οποίο μπορούμε να γράψουμε και έχει ως κωδικοποίηση το utf-8.\n",
    "- Μέσα σε αυτό το αρχείο μορφής json αποθηκεύονται τα δεδομένα της λίστας articles.\n",
    "- Τέλος, τυπώνεται σχετικό μήνυμα επιτυχίας."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://en.wikipedia.org/wiki/Processor_(computing)\n",
      "Fetching: https://en.wikipedia.org//wiki/Processor_(computing)\n",
      "Fetching: https://en.wikipedia.org//wiki/Processor_(disambiguation)#Computing\n",
      "Fetching: https://en.wikipedia.org//wiki/Computing\n",
      "Fetching: https://en.wikipedia.org//wiki/Computer_science\n",
      "Fetching: https://en.wikipedia.org//wiki/Circuit_(computer_science)\n",
      "Fetching: https://en.wikipedia.org//wiki/Memory_(computing)\n",
      "Fetching: https://en.wikipedia.org//wiki/Microprocessor\n",
      "Fetching: https://en.wikipedia.org//wiki/Metal%E2%80%93oxide%E2%80%93semiconductor\n",
      "Fetching: https://en.wikipedia.org//wiki/Integrated_circuit\n",
      "The data has been saved to \"articles.json\"\n"
     ]
    }
   ],
   "source": [
    "start_url = \"https://en.wikipedia.org/wiki/Processor_(computing)\"\n",
    "articles = fetch_articles(start_url, num_articles = 10)\n",
    "\n",
    "with open(\"articles.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(articles, file, ensure_ascii=False, indent=4)\n",
    "print(\"The data has been saved to \\\"articles.json\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Βήμα 2. Προεπεξεργασία κειμένου (Text Processing):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Εισαγωγή των βιβλιοθηκών\n",
    "- Εισάγουμε την βιβλιοθήκη pandas γιατί την χρειαζόμαστε για την επεξεργασία των δεδομένων από αρχεία σε μορφή DataFrame.\n",
    "- Η βιβλιοθήκη nltk είναι απαραίτητη, καθώς παρέχει αρκετά εργαλεία για την επεξεργασία και την ανάλυση της φυσικής γλώσσας.\n",
    "- Η nltk.tokenize παρέχει την βιβλιοθήκη word_tokenize η οποία χρησιμοποιείται για να υλοποιήσει την διαδικασία του Tokenization, δηλαδή για να χωρίσει το κείμενο σε λέξεις ή αλλιώς tokens.\n",
    "- Η nltk.corpus παρέχει την stopwords, όπου σκοπός της είναι να μας παρέχει τις κοινές λέξεις που υπάρχουν σε ένα κείμενο, οι οποίες συνήθως δεν έχουν σημασιολογική βαρύτητα, όπως 'and', 'but', 'the'.\n",
    "- Η nltk.stem παρέχεις τις PorterStemmer και WordNetLemmatizer. Από την PorterStemmer χρησιμοποιώντας το Stemming μπορούμε για κάθε λέξη να αφαιρέσουμε την περιττή κατάληξη τους, πχ. το 'programs' γίνεται 'program'. Επίσης, από την WordNetLemmatizer χρησιμοποιώντας το Lemmatization παίρνουμε την κανονική μορφή μιας λέξης, πχ. το 'worse' γίνεται 'bad'.\n",
    "- Η βιβλιοθήκη string παρέχει διάφορους χαρακτήρες στίξης, την χρησιμοποιύμε έτσι ώστε να μπορέσουμε να αφαιρέσουμε από το κείμενο τα σημεία στίξης που υπάρχουν."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Με την χρήση του nltk.download κατεβάζουμε τους απαίτητους πόρους για να επεξεργαστούμε κατάλληλα το κείμενο όπως επιθυμούμε.\n",
    "- Το punkt χρειάζεται για το Tokenization.\n",
    "- Το stopwords μας παρέχει τις λίστες με τα stop words.\n",
    "- Το wordnet είναι απαραίτητο για το Lemmatization, διότι παρέχει την βάση δεδομένων των λεξικών που θα χρησιμοποιήσουμε."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Kat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Σκοπός της συνάρτησης process_content είναι να δέχεται ένα κείμενο και να το προετοιμάζει για παραπάνω ανάλυση στα επόμενα βήματα, αφαιρώντας πρώτα τους ειδικούς χαρακτήρες, τα stop words και να εφαρμόζει Lemmatization.\n",
    "- Αρχικά, για να αποφύγουμε σφάλματα ελέγχουμε αν η παράμετρος text είναι τύπου string.\n",
    "- Εφαρμόζουμε το Tokenization στο text και αρχικοποιούμε μια λίστα processed_words στην οποία θα αποθηκεύονται οι λέξεις που έχουν επεξεργαστεί.\n",
    "- Χρησιμοποιούμε μια δομή επανάληψης for με σκοπό να επεξεργαστούμε κάθε λέξη από την λίστα words.\n",
    "- Ελέγχουμε αν η λέξη είναι αριθμός και αν ναι τότε την προσθέτουμε κατευθείαν στην λίστα processed_words χωρίς καμία επεξεργασία.\n",
    "- Αν η λέξη είναι συμβολοσειρά τότε με την βοήθεια της string.punctuation, η οποία περιέχει όλους τους χαρακτήρες στίξης, τους αφαιρούμε από την αρχή ή το τέλος της λέξης με την χρήση της strip(). Στη συνέχεια, υλοποιούμε την διαδικασία του Lemmatization, δηλαδή με το WordNetLemmatizer μετατρέπουμε την λέξη στην βασική της μορφή. Επίσης, για την διαδικασία της αφαίρεσης των stop words ελέγχουμε αν η λέξη σε πεζά γράμματα δεν ανήκει στα στο σύνολο των stop words και αν όχι τότε την προσθέτουμε στην λίστα με τις επεξεργασμένες λέξεις.\n",
    "- Τέλος, χρησιμοποιούμε την join() για να ενώσουμε όλες τις επεξεργασμένες λέξεις με κενά ανάμεσα σε μία συμβολοσειρά και το επιστρέφουμε."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    # Tokenization\n",
    "    words = word_tokenize(text)\n",
    "    processed_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            processed_words.append(word)\n",
    "        else:\n",
    "            # Αφαίρεση ειδικών χαρακτήρων\n",
    "            word = word.strip(string.punctuation)\n",
    "            \n",
    "            # Lemmatization\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "            \n",
    "            # Αφαίρεση stop words\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            if word.lower() not in stop_words:\n",
    "                processed_words.append(word.lower())\n",
    "    \n",
    "    return ' '.join(processed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Διαβάζουμε το αρχείο articles.json και το μετρατρέπουμε σε DataFrame, εκτυπώνοντας σχετικό μήνυμα αν παρουσιαστεί σφάλμα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dataframe = pd.read_json(\"articles.json\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading JSON file: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Για να είμαστε σίγουροι ότι τα κείμενα μας έχουν επεξεργαστεί σωστά θα τα ελέγξουμε εφαρμόζοντας σε κάθε στήλη του DataFrame την process_content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in dataframe.columns:\n",
    "    dataframe[column] = dataframe[column].apply(process_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Μετονομάζουμε τις στήλες του DataFrame για να είναι πιο κατανοητές και φιλικές."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = {\n",
    "    dataframe.columns[0]: 'Title',\n",
    "    dataframe.columns[1]: 'Url',\n",
    "    dataframe.columns[2]: 'Content'\n",
    "}\n",
    "dataframe = dataframe.rename(columns=new_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αποθηκεύουμε το επεξεργασμένο πλέον DataFrame στο καινούργιο αρχείο processed.json και εκτυπώνουμε σχετικό μήνυμα επιτυχίας."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_json(\"processed.json\", orient='records', indent=4)\n",
    "\n",
    "print(\"The data has been saved to \\\"processed.json\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Βήμα 3. Ευρετήριο (indexing): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Εισάγουμε τις βιβλιοθήκες:\n",
    "- json για να φορτώσουμε τα δεδομένα του αρχείου processed.json που δημιουργήσαμε στο προηγούμενο βήμα και\n",
    "- την defaultdict από την collections έτσι ώστε  να δημιουργήσουμε ένα λεξικό το οποίο θα  περιέχει λίστες αυτόματα χωρίς να χρειάζεται να το ελέγχουμε εμείς."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ανοίγουμε το αρχείο processed.json σε λειτουργία ανάγνωσης ('r'), διαβάζουμε το περιεχόμενο και το μετατρέπουμε σε ένα λεξικό με μορφή λίστας, το οποίο ποθηκεύουμε στην μεταβλητή documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed.json', 'r') as json_file:\n",
    "    documents = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Δημιουργούμε ένα αντίστροφο ευρετήριο, δηλαδή ένα λεξικό."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = defaultdict(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Χρησιμοποιούμε μία δομή επανάληψης for η οποία τρέχει για κάθε έγγραφο της λίστας documents.\n",
    "- Χρησιμοποιούμε άλλη μία for για κάθε πεδίο των εγγράφων, με την βοήθεια της document.items() επιστρέφονται τα πεδία που υπάρχουν με τις τιμές τους, π.χ. Title.\n",
    "- Ελέγχει αν το field_value είναι συμβολοσειρά και αν ναι τότε αυτό το πεδίο χωρίζεται σε ξεχωριστές λέξεις με την split() και αποθηκεύεται στην λίστα terms. Αυτό το κάνουμε με σκοπό να μην συμπεριλάβουμε τα πεδία που δεν περιέχουν σημασιολογικό κείμενο όπως το URL.\n",
    "- Ακόμη, τρέχουμε ακόμη μία for για να διατρέξουμε την κάθε λέξη μέσα στην λίστα terms. Ελέγχουμε αν το doc_id υπάρχει ήδη στο inverted_index για τον συγκεκριμένο όρο και αν όχι τότε το προσθέτουμε στο ευρετήριο. Σκοπός αυτού του βήματος είναι να αποφύγουμε την καινούργια εγγραφή μιας ήδη υπάρχουσας λέξης στο ίδιο έγγραφο."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_id, document in enumerate(documents):\n",
    "    for field, field_value in document.items():\n",
    "        if isinstance(field_value, str):\n",
    "            terms = field_value.split()\n",
    "            \n",
    "            for term in terms:\n",
    "                if doc_id not in inverted_index[term]:\n",
    "                    inverted_index[term].append(doc_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Το αρχείο inverted_index.json ανοίγει σε λειτουργία εγγραφής ('w') και αποθηκεύει το inverted_index μέσα σε αυτό. Τέλος, ευμφανίζεται μήνυμα επιτυχίας προς τον χρήστη."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inverted_index.json', 'w') as json_file:\n",
    "    json.dump(inverted_index, json_file, indent=4)\n",
    "\n",
    "print(\"The data has been saved to \\\"inverted_index.json\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Βήμα 4. Μηχανή αναζήτησης (Search Engine):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " α) Επεξεργασία ερωτήματος (Query Processing): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αρχικά εισάγουμε την βιβλιοθήκη json, γιατί θα την χρειαστούμε για να φορτώσουμε και να χρησιμοποιήσουμε στην αναζήτηση το αρχείο inverted_index.json από το προηγούμενο ερώτημα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Εδώ η συνάρτηση load_inverted_index χρηιμοποιείται για να φορτώσουμε το ανεστραμμένο ευρετήριο από το αρχείο inverted_index.json. Με την open() διαβάζουμε το αρχείο και με την json.load(f) φορτώνουμε το περιεχόμενο του επιστρέφοντας το σε μορφή λεξικού."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_inverted_index(filepath):\n",
    "    with open(filepath, \"r\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Σκοπός της συνάρτησης boolean_search είναι να γίνεται αναζήτηση των όρων σε όλα τα κείμενα με την χρήση των λογικών τελεστών AND, OR και NOT.\n",
    "- Ξεκινώντας, μετατρέπουμε το ερώτημα που θα μας δώσει ο χρήστης σε πεζά γράμματα και διαχωρίζουμε τις λέξεις με κενά βάζοντας το αποτέλεσμα στο terms.\n",
    "- Αν ο χρήστης δεν εισάγει τίποτα τότε η συνάρτηση επιστρέφει ένα κενό σύνολο.\n",
    "- Στη συνέχεια, με την χρήση get() προσπαθούμε να βρούμε ποια έγγραφα περιέχουν το συγκεκριμένο στοιχείο ξεκινώντας από το πρώτο και το αποθηκεύουμε στο result_set. Αν δεν υπάρχει στο ευρετήριο τότε αποθηκεύεται μια κενή λίστα.\n",
    "- Μετά ελέγχουμε τους υπόλοιπους όρους και τους τελεστές που ζήτησε ο χρήστης.\n",
    "- Αν υπάρχει τελεστής στο ερώτημα αλλά δεν υπάρχει όρος μετά από αυτόν, εμφανίζεται σχετικό μήνυμα σφάλματος.\n",
    "- Έπειτα, κάνουμε την ίδια διαδικασία πάλι για τον επόμενο όρο της αναζήτησης και βρίσκουμε το έγγραφα που τον περιέχουν.\n",
    "- Με την δομή επιλογής if αν ο χρήστης επέλεξε τον τελεστή and τότε με την χρήση του result_set.intersection() κρατάμε μόνο τα κοινά έγγραφα στα οποία περιέχονται οι όροι, στην επιλογή or με την result_set.union() ενώνουμε τα έγγραφα που περιέχουν τουλάχιστον έναν από τους όρους, στην επιλογή not με την result_set.difference() αφαιρούμε τα έγγραφα που δεν περιέχουν τους όρους και σε κάθε άλλη περίπτωση εμφανίζεται σχετικό μήνυμα λάθους.\n",
    "- Η επανάληψη συνεχίζεται προχωρώντας το index i κατά δύο θέσεις για να εκτελέσουμε την διαδικασία για τον επόμενο τελεστή.\n",
    "- Τέλος, επιστρέφουμε τα αποτελέσματα της αναζήτησης με το σύνολο result_set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_search(query, inverted_index):\n",
    "    terms = query.lower().split()\n",
    "    if not terms:\n",
    "        return None, \"Το ερώτημα είναι κενό.\"\n",
    "\n",
    "    if len(terms) % 2 == 0:\n",
    "        return None, f\"Λάθος: Λείπει όρος μετά τον τελεστή '{terms[-1]}'.\"\n",
    "\n",
    "    result_set = set(inverted_index.get(terms[0], []))\n",
    "    \n",
    "    i = 1\n",
    "    while i < len(terms):\n",
    "        operator = terms[i]  #AND, OR, NOT\n",
    "        \n",
    "        next_term = terms[i + 1]\n",
    "        next_docs = set(inverted_index.get(next_term, []))\n",
    "        \n",
    "        # Εκτέλεση της λειτουργίας\n",
    "        if operator == \"and\":\n",
    "            result_set = result_set.intersection(next_docs)\n",
    "        elif operator == \"or\":\n",
    "            result_set = result_set.union(next_docs)\n",
    "        elif operator == \"not\":\n",
    "            result_set = result_set.difference(next_docs)\n",
    "        else:\n",
    "            return None, f\"Λάθος: Άγνωστος τελεστής '{operator}'.\"\n",
    "        \n",
    "        i += 2\n",
    "    \n",
    "    return result_set, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Βρισκόμαστε στην main, η οποία διαχειρίζεται την διαδικασία της αναζήτησης.\n",
    "- Σε αυτήν φορτώνουμε το ανεστραμμένο ευρετήριο inverted_index.json και το αποθηκεύουμε στο filepath, καθώς θα χρειαστεί σας παράμετρο για την συνάρτηση load_inverted_index.\n",
    "- Εκτυπώνουμε βασικά μηνύματα καλωσορίσματος φιλικά προς τον χρήστη και του εξηγούμε πως να χρησιμοποιήσει την αναζήτηση.\n",
    "- Με την χρήση ενός ατελείωτου βρόχου ζητάμε να εισάγει το ερώτημα του το οποίο αποθηκεύεται στο query. Για να σταματήσει την αναζήτηση πληκτρολογεί exit.\n",
    "- Εκτελούμε την αναζήτηση με την boolean_search και αποθηκεύουμε τα αποτελέσματα στο results.\n",
    "- Τέλος, αν υπάρχει error τότε τερματίζει το πρόγραμμα, αν βρεθούν τα αποτελέσματα τα εμφανίζουμε, αλλιώς εμφανίζεται σχετικό μήνυμα για την περίπτωη που δεν βρέθηκαν αποτελέσματα για το ερώτημα του χρήστη."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    filepath = \"inverted_index.json\"  \n",
    "    inverted_index = load_inverted_index(filepath)\n",
    "\n",
    "    print(\"Καλωσήρθατε στη Μηχανή Αναζήτησης!\")\n",
    "    print(\"Πληκτρολογήστε το ερώτημά σας χρησιμοποιώντας Boolean τελεστές (AND, OR, NOT) ή 'exit' για έξοδο.\")\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"\\nΕρώτημα: \").strip()\n",
    "        \n",
    "        if query.lower() == \"exit\":\n",
    "            print(\"Έξοδος από τη Μηχανή Αναζήτησης!\")\n",
    "            break\n",
    "        \n",
    "        results, error = boolean_search(query, inverted_index)\n",
    "        \n",
    "        if error:\n",
    "            print(error)\n",
    "        elif results:\n",
    "            print(f\"Αποτελέσματα για το ερώτημα '{query}': {results}\")\n",
    "        else:\n",
    "            print(f\"Δεν βρέθηκαν αποτελέσματα για το ερώτημα '{query}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Διασφαλίζουμε ότι η main θα εκτελεστεί μόνο όταν το αρχείο τρέχει απευθείας."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "β) Κατάταξη αποτελεσμάτων (Ranking):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Σκοπός αυτού του προγράμματος είναι να παρέχει στον χρήστη τρεις τρόπους αναζήτησης: Boolean Search, TF-IDF Ranking και BM25 Ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Για να τρέξει το πρόγραμμα πρέπει να εγκατασταθεί η βιβλιοθήκη rank-bm25 με την παρακάτω εντολή."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install rank-bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Γίνεται εισαγωγή των βιβλιοθηκών που χρειαζόμαστε:\n",
    "- Η TfidfVectorizer από την sklearn.feature_extraction.text είναι χρήσιμη για τον αλγόριθμο TF-IDF, καθώς μπορεί να μετατρέψει τα κείμενα σε αριθμητικά διανύσματα βάσει του πόσο συχνά εμφανίζονται οι λέξεις.\n",
    "- Χρησιμοποιούμε την cosine_similarity από την sklearn.metrics.pairwise πάλι για τον αλγόριθμο TF-IDF. Μπορεί να μας παρέχει με την ομοιότητα δύο διανυσμάτων.\n",
    "- Η BM25Okapi από την rank_bm25 είναι αναγκαία για την υλοποίηση του αλγορίθμου BM25.\n",
    "- Η json χρειάζεται για την φόρτωση αρχείων μορφής json.\n",
    "- Η numpy έχει διάφορες μαθηματικές λειτουργίας, όπως η ταξινόμηση των εγγράφων με βάση τα σκορ που μας είναι χρήσιμη στο συγκεκριμένο πρόγραμμα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Σκοπός της παρακάτω συνάρτησης είναι να γίνει η φόρτωση των εγγράφων από το αρχείο μορφής json που περιέχεται στο filepath.\n",
    "- Ανοίγουμε το αρχείο json, διαβάζουμε τα δεδομένα του και τα βάζουμε στην μεταβλητή data.\n",
    "- Αν η data περιέχει λίστα, τότε επιστρέφουμε μόνο το πεδίο του content.\n",
    "- Αν περιέχει λεξικό το επιστρέφουμε όπως είναι.\n",
    "- Αν έχει κάποια διαφορετική δομή, τότε εμφανίζεται μήνυμα λάθους."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "        if isinstance(data, list):\n",
    "            return [doc[\"content\"] for doc in data if \"content\" in doc]\n",
    "        elif isinstance(data, dict):\n",
    "            return data\n",
    "        else:\n",
    "            raise ValueError(\"Το αρχείο δεν έχει υποστηριζόμενη μορφή.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Χρησιμοποιούμε την συνάρτηση boolean_search για να υλοποιήσουμε τον αντίστοιχο αλγόριθμο αναζήτησης για τους τελεστές AND, OR και NOT όπως κάναμε στο βήμα 4α. Σκοπός της είναι να ελέγξει αν τα έγγραφα περιέχουν τους όρους που δίνει ο χρήστης και να επιστρέψει αυτό το σύνολο με τα αποτελέσματα με την result_set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boolean Search\n",
    "def boolean_search(query, inverted_index):\n",
    "    terms = query.lower().split()\n",
    "    if not terms:\n",
    "        return set()\n",
    "\n",
    "    result_set = set(inverted_index.get(terms[0], []))\n",
    "    i = 1\n",
    "    while i < len(terms):\n",
    "        operator = terms[i]\n",
    "        if i + 1 >= len(terms):\n",
    "            break\n",
    "        next_term = terms[i + 1]\n",
    "        next_docs = set(inverted_index.get(next_term, []))\n",
    "        if operator == \"and\":\n",
    "            result_set = result_set.intersection(next_docs)\n",
    "        elif operator == \"or\":\n",
    "            result_set = result_set.union(next_docs)\n",
    "        elif operator == \"not\":\n",
    "            result_set = result_set.difference(next_docs)\n",
    "        i += 2\n",
    "    return result_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Σκοπός της TF-IDF Ranking είναι να μετρήσει πόσο σημαντική είναι η λέξη μέσα σε ένα σύνολο εγγράφων με βάση την συχνότητα τους μέσα σε αυτά.\n",
    "- Έτσι, μετατρέπει τα έγγραφα και το ερώτημα του χρήστη σε αριθμητικά διανύσματα μέσω της TfidfVectorizer() και της transform().\n",
    "- Χρησιμοποιώντας την cosine_similarity() υπολογίζει την ομοιότητα μεταξύ του ερωτήματος και των εγγράφων.\n",
    "- Τέλος, επιστρέφει τα έγγραφα με ταξινόμηση βάση την σχετικότητα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF Ranking\n",
    "def tfidf_ranking(query, documents):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    doc_vectors = vectorizer.fit_transform(documents)\n",
    "    query_vector = vectorizer.transform([query.lower()])\n",
    "    scores = cosine_similarity(query_vector, doc_vectors).flatten()\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    return [(idx, scores[idx]) for idx in sorted_indices if scores[idx] > 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Η συνάρτηση bm25_ranking, όπως και η tfidf_ranking ταξινομεί τα έγγραφα με βάση την σχετικότητα, όμως το BM25 μετράει και το μήκος των εγγράφων μαζί με την συχνότητα των λέξεων.\n",
    "- Δηλαδή το περιεχόμενο του documents και αυτό του ερωτήματος χωρίζονται σε λέξεις όπου υπάρχει κενό με χρήση της split().\n",
    "- Χρησιμοποιώντας την βιβλιοθήκη rank_bm25 υπολογίζουμε για κάθε έγγραφο ένα BM25 score. Αυτό το score δείχνει την σχετικότητα του εγγράφου με το ερώτημα του χρήστη και επιστρέφεται ως μία λίστα με αυτές τις βαθμολογίες.\n",
    "- Η συνάρτηση επιστρέφει μία λίστα στην οποία περιέχεται το index του εγγράφου στην λίστα documents και την αντίστοιχη βαθμολογία."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BM25 Ranking\n",
    "def bm25_ranking(query, documents):\n",
    "    tokenized_docs = [doc.split() for doc in documents]\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "    tokenized_query = query.split()\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    return [(idx, scores[idx]) for idx in sorted_indices if scores[idx] > 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Σκοπός της main είναι να δημιουργεί την κατάλληλη αλληλεπίδραση μεταξύ του εργαλείου αναζήτησης και του χρήστη με χρήση σχετικών μηνυμάτων για την αποδοτική κατανόηση του. Ρόλος της είναι να φορτώσει τα κατάλληλα αρχεία json που θα χρησιμοποιηθούν στην αναζήτηση και να παρέχει μενού επιλογών με σχετικές οδηγίες."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    filepath_docs = \"articles.json\"\n",
    "    filepath_index = \"inverted_index.json\"\n",
    "    documents = load_documents(filepath_docs)\n",
    "    inverted_index = load_documents(filepath_index)\n",
    "\n",
    "    print(\"Επιλέξτε αλγόριθμο ανάκτησης:\")\n",
    "    print(\"1. Boolean Retrieval\")\n",
    "    print(\"2. TF-IDF Ranking\")\n",
    "    print(\"3. BM25 Ranking\")\n",
    "    print(\"Πληκτρολογήστε 'exit' για έξοδο.\")\n",
    "\n",
    "    while True:\n",
    "        choice = input(\"\\nΕπιλογή: \").strip()\n",
    "        if choice.lower() == \"exit\":\n",
    "            print(\"Έξοδος από τη Μηχανή Αναζήτησης!\")\n",
    "            break\n",
    "\n",
    "        query = input(\"Ερώτημα: \").strip()\n",
    "        if not query:\n",
    "            print(\"Το ερώτημα δεν μπορεί να είναι κενό.\")\n",
    "            continue\n",
    "\n",
    "        if choice == \"1\":   #Boolean Retrieval\n",
    "            results = boolean_search(query, inverted_index)\n",
    "            if results:\n",
    "                print(\"\\nBoolean Results:\")\n",
    "                for idx in results:\n",
    "                    if idx < len(documents):\n",
    "                        print(f\"Έγγραφο {idx}\")\n",
    "                    else:\n",
    "                        print(f\"Το έγγραφο {idx} δεν υπάρχει στο articles.json.\")\n",
    "            else:\n",
    "                print(\"Δεν βρέθηκαν αποτελέσματα.\")\n",
    "\n",
    "        elif choice == \"2\":  #TF-IDF Ranking\n",
    "            results = tfidf_ranking(query, documents)\n",
    "            print(\"\\nTF-IDF Results:\")\n",
    "            for idx, score in results:\n",
    "                if idx < len(documents):\n",
    "                    print(f\"Έγγραφο {idx} (Σκορ: {score:.4f})\")\n",
    "                else:\n",
    "                    print(f\"Το έγγραφο {idx} δεν υπάρχει στο articles.json.\")\n",
    "\n",
    "        elif choice == \"3\":  #BM25 Ranking\n",
    "            results = bm25_ranking(query, documents)\n",
    "            print(\"\\nBM25 Results:\")\n",
    "            for idx, score in results:\n",
    "                if idx < len(documents):\n",
    "                    print(f\"Έγγραφο {idx} (Σκορ: {score:.4f})\")\n",
    "                else:\n",
    "                    print(f\"Το έγγραφο {idx} δεν υπάρχει στο articles.json.\")\n",
    "\n",
    "        else:\n",
    "            print(\"Μη έγκυρη επιλογή. Παρακαλώ προσπαθήστε ξανά.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
