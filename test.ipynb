{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://en.wikipedia.org/wiki/Processor_(computing)\n",
      "Fetching: https://en.wikipedia.org//wiki/Processor_(computing)\n",
      "Fetching: https://en.wikipedia.org//wiki/Processor_(disambiguation)#Computing\n",
      "Fetching: https://en.wikipedia.org//wiki/Computing\n",
      "Fetching: https://en.wikipedia.org//wiki/Computer_science\n",
      "Fetching: https://en.wikipedia.org//wiki/Circuit_(computer_science)\n",
      "Fetching: https://en.wikipedia.org//wiki/Memory_(computing)\n",
      "Fetching: https://en.wikipedia.org//wiki/Microprocessor\n",
      "Fetching: https://en.wikipedia.org//wiki/Metal%E2%80%93oxide%E2%80%93semiconductor\n",
      "The data has been saved to \"articles.json\"\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "#Συνάρτηση για να ανακτήσουμε συγκεκριμένα άρθρα\n",
    "def fetch_articles(start_url, num_articles = 10):\n",
    "    base_url = \"https://en.wikipedia.org/\"\n",
    "    visited = set() #Σύνολο url τα οποία έχουμε ήδη επισκεφτεί\n",
    "    articles = [] #Δεδομένα των άρθρων\n",
    "    \n",
    "    #Συνάρτηση για να συλλέξουμε τα δεδομένα της κάθε σελίδας\n",
    "    def fetch_article_data(url):\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        title = soup.find('h1').text\n",
    "        paragraphs = soup.find_all('p')\n",
    "        content = \" \".join([p.text for p in paragraphs])\n",
    "        return {\"title\": title, \"url\": url, \"content\": content}\n",
    "    \n",
    "    #Τα url τα οποία θα επισκεφτούμε στην συνέχεια\n",
    "    to_visit = [start_url]\n",
    "    while to_visit and len(articles) < num_articles:\n",
    "        current_url = to_visit.pop(0)\n",
    "        if current_url in visited:\n",
    "            continue\n",
    "        visited.add(current_url)\n",
    "        \n",
    "        #Αποφεύγουμε το Main Page της Wikipedia\n",
    "        if 'Main_Page' in current_url:\n",
    "            continue\n",
    "        \n",
    "        print(f\"Fetching: {current_url}\")\n",
    "        article = fetch_article_data(current_url)\n",
    "        if article:\n",
    "            articles.append(article)\n",
    "        \n",
    "        response = requests.get(current_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for link in soup.find_all('a', href = True):\n",
    "            href = link['href']\n",
    "            if href.startswith('/wiki/') and ':' not in href:\n",
    "                full_url = base_url + href\n",
    "                if full_url not in visited:\n",
    "                    to_visit.append(full_url)\n",
    "        \n",
    "    return articles\n",
    "        \n",
    "\n",
    "start_url = \"https://en.wikipedia.org/wiki/Processor_(computing)\"\n",
    "articles = fetch_articles(start_url, num_articles = 9)\n",
    "\n",
    "with open(\"articles.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(articles, file, ensure_ascii=False, indent=4)\n",
    "print(\"The data has been saved to \\\"articles.json\\\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
